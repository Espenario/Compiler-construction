% Лабораторная работа № 2.3 «Синтаксический анализатор на основе
  предсказывающего анализа»
% 18 апреля 2023 г.
% Головкин Дмитрий, ИУ9-62Б

# Цель работы
Целью данной работы является изучение алгоритма построения таблиц предсказывающего анализатора.

# Индивидуальный вариант
' аксиома
<axiom <E>>
' правила грамматики
<E    <T E'>>
<E'   <+ T E'> <>> 
<T    <F T'>>
<T'   <* F T'> <>>
<F    <n> <( E )>>

# Реализация

## Неформальное описание синтаксиса входного языка
В начале тегом axiom задается аксиома грамматики. При этом все правила, как и все нетерминалы,
 обрамлены угловыми скобочками (включая пустой символ). Также допустимо использование комментариев, 
 которые ограничиваются концом строки и начинаются с символа '. В качестве разделителя между правилами
 испольуется перевод строки. Разделителем между левой и правой частями правила является символ табуляции.

## Лексическая структура
axiom Name
list Rules
Rule = lhs \t rhs

## Грамматика языка
<Grammar> :: AXIOM <Rules>
<Rules> :: <Rule> <RestRules>
<RestRules> :: <Rule> <RestRules> | Epsilon
<Rule> :: <Lhs> <Rhs>
<Rhs> :: <RhsPart> <RhsRestParts>
<RhsPart> :: Term | NonTerm
<Lhs> :: NonTerm
<RhsRestParts> :: <RhsPart> <RhsRestParts> | Epsilon

## Программная реализация

```python
import re
from dataclasses import dataclass
from queue import Queue

class Lex_Tags():
    Tag_ERROR = 1
    Tag_TERMINAL = 2
    Tag_NONTERMINAL = 3
    Tag_AXIOM = 4
    Tag_COMMENT = 5
    Tag_OPEN_GROUP = 6
    Tag_CLOSE_GROUP = 7
    Tag_UNMATCHED = 8
    Tag_END = 9


class Token:
    
    def __init__(self, tag = None, value = ""):
        self.tag = tag
        self.text = value

class CoordsToken(Token):

    def __init__ (self, tag = None, value = "", index_start = 0, index_end = 0):
        self.tag = tag
        self.text = value
        self.index_start = index_start
        self.index_end = index_end

    def __repr__(self):
        return str(self.tag) + '(' + str(self.index_start) + ',' + str(self.index_end) + '):' + str(self.text)

class Lexer:

    def token_matcher(self, text_input):

        matched = re.match(r"'.*\n", text_input)
        if matched:
            return Token(tag=Lex_Tags.Tag_COMMENT, value=matched.group())
        matched = re.match(r"<axiom <({})>>\n".format(r"[A-Z]'?"), text_input)
        if matched:
            return Token(tag=Lex_Tags.Tag_AXIOM, value=matched.group())
        matched = re.match("<", text_input)
        if matched:
            return Token(tag=Lex_Tags.Tag_OPEN_GROUP, value=matched.group())
        matched = re.match(">", text_input)
        if matched:
            return Token(tag=Lex_Tags.Tag_CLOSE_GROUP, value=matched.group())
        matched = re.match(r"[a-z\(\)\+\*]",text_input)
        if matched:
            return Token(tag=Lex_Tags.Tag_TERMINAL, value=matched.group())
        matched = re.match(r"[A-Z]'?", text_input)
        if matched:
            return Token(tag=Lex_Tags.Tag_NONTERMINAL, value=matched.group())

        return Token(tag=Lex_Tags.Tag_UNMATCHED, value=text_input[0])

    def perform_tokenize(self, text_input):
        new_tokens = Queue()
        idx = 0

        while idx < len(text_input):
            token = self.token_matcher(text_input[idx:])
            if token.tag == Lex_Tags.Tag_UNMATCHED and token.text.isspace() or token.tag == Lex_Tags.Tag_COMMENT:
                idx += len(token.text)
            else:
                if token.tag == Lex_Tags.Tag_AXIOM:
                    axiom_value = re.search(r"[A-Z]'?", text_input).group(0)
                    t = CoordsToken(Lex_Tags.Tag_AXIOM, axiom_value, idx, idx + len(token.text))
                    new_tokens.put(CoordsToken(Lex_Tags.Tag_AXIOM, axiom_value, idx, idx + len(token.text)))
                else:
                    new_tokens.put(CoordsToken(token.tag, token.text, idx, idx + len(token.text)))
                idx += len(token.text)

        new_tokens.put(CoordsToken(Lex_Tags.Tag_END, "", idx+1, idx+1))
        return new_tokens


class Nt:
    def __init__(self, str, id=0):
        self.value = str
        self.num_rule = id
        self.accessors = []

    def __hash__(self):
        return hash(self.value)

    def __str__(self):
        return self.value
    
    def print(self, indent):
        print(indent + str(self) + ":")
        for child in self.accessors:
            child.print(indent + "\t")

    def __eq__(self, other):
        return (isinstance(other, Nt) and
                self.value == other.value)


class T:
    def __init__(self, str):
        self.value = str

    def __hash__(self):
        return hash(self.value)

    def __str__(self):
        return self.value
    
    def print(self, indent):
        print(indent + str(self))

    def __eq__(self, other):
        return (isinstance(other, T) and
                self.value == other.value)
    
    
class Leaf:
    def __init__(self, value):
        self.value = value
      
    def print(self, indent=""):
        print(str(indent) + 'End Node:' + str(self.value))

class Inner:
    def __init__(self, non_term, id):
        self.non_term = non_term
        self.rule_id = id
        self.accessors = []
      
    def print(self, indent=""):
        print(indent + 'Node:' + str(self.non_term) + ', value:' + str(self.rule_id))
        for child in self.accessors:
            child.print(indent + "\t")

class Rules_in_Table:
    def __init__(self):
        self.rule_ids = iter(range(100))
        self.rules = [(Nt("Programm"), Lex_Tags.Tag_AXIOM), (Nt("LR"), Lex_Tags.Tag_OPEN_GROUP),
                      (Nt("LR"), Lex_Tags.Tag_OPEN_GROUP), (Nt("RLR"), Lex_Tags.Tag_END),
                      (Nt("OR"), Lex_Tags.Tag_OPEN_GROUP), (Nt("SR"), Lex_Tags.Tag_OPEN_GROUP),
                      (Nt("SRLR"), Lex_Tags.Tag_OPEN_GROUP), (Nt("SRLR"), Lex_Tags.Tag_CLOSE_GROUP),
                      (Nt("SRLR"), Lex_Tags.Tag_END), (Nt("SRT"), Lex_Tags.Tag_OPEN_GROUP),
                      (Nt("RLRTS"), Lex_Tags.Tag_T), (Nt("RLRTS"), Lex_Tags.Tag_Nt),
                      (Nt("RLRTS"), Lex_Tags.Tag_CLOSE_GROUP), (Nt("RLRTS"), Lex_Tags.Tag_END),
                      (Nt("RSF"), Lex_Tags.Tag_T), (Nt("RSF"), Lex_Tags.Tag_Nt),
                      (Nt("RSF"), Lex_Tags.Tag_CLOSE_GROUP)]
        self.rhs_rules = [([T("Stmt_Axiom"), Nt("LR")], next(self.rule_ids)),
                          ([Nt("OR"), Nt("RLR")], next(self.rule_ids)),
                          ([Nt("OR"), Nt("RLR")], next(self.rule_ids)),
                          ([], next(self.rule_ids)),
                          ([T("OpenStmt"), T("NtermStmt"), Nt("SR"), T("CloseStmt")], next(self.rule_ids)),
                          ([Nt("SRT"), Nt("SRLR")], next(self.rule_ids)),
                          ([Nt("SRT"), Nt("SRLR")], next(self.rule_ids)),
                          ([], next(self.rule_ids)),
                          ([], next(self.rule_ids)),
                          ([T("OpenStmt"), Nt("RSF"), Nt("RLRTS"), T("CloseStmt")], next(self.rule_ids)),
                          ([Nt("RSF"), Nt("RLRTS")], next(self.rule_ids)),
                          ([Nt("RSF"), Nt("RLRTS")], next(self.rule_ids)),
                          ([], next(self.rule_ids)),
                          ([], next(self.rule_ids)),
                          ([T("Term")], next(self.rule_ids)),
                          ([T("NtermStmt")], next(self.rule_ids)),
                          ([], next(self.rule_ids))
        ]

test_str = """' аксиома
<axiom <E>>
' правила грамматики
<E  <T E'>>
' и это комментарий
<E' <+ T E'> <>> 
<T  <F T'>>
<T' <* F T'> 'и это комментарий

 <>>
<F  <n> <( E )>>"""

def top_down(tokens):
    type_mapping = {
        Lex_Tags.Tag_AXIOM: "Stmt_Axiom",
        Lex_Tags.Tag_TERMINAL: "Term",
        Lex_Tags.Tag_NONTERMINAL: "NtermStmt",
        Lex_Tags.Tag_OPEN_GROUP: "OpenStmt",
        Lex_Tags.Tag_CLOSE_GROUP: "CloseStmt"
    }

    delta = Rules_in_Table()
    sparent = Inner(None, None)
    stack = [(sparent, Terminal('$')), (sparent, Nonterminal('Programm'))]

    token = tokens.get()
    parent, X = stack.pop()
    
    while X.value != '$':
        if isinstance(X, Terminal):
            if X.value == type_mapping[token.tag]:
                parent.accessors.append(Leaf(token))
                token = tokens.get()
            else:
                raise ValueError(f"T. Ожидался {X}, Получен {token}")
        elif (X, token.tag) in delta.rules:
            inner = Inner(X, delta.rhs_rules[delta.rules.find[X, token.tag][1]])
            parent.accessors.append(inner)
            for elem in delta.rhs_rules[delta.rules.find[X, token.tag]][0][::-1]:
                stack.append((inner, elem))
        else:
            raise ValueError(f"Ожидался {X}, Получен {token}")
        
        parent, X = stack.pop()

    return sparent.accessors[0]

lexer = Lexer()

tokens = lexer.perform_tokenize(test_str)

v = top_down(tokens)

v.print()
```

Вывод на `stdout`

```
Node:Programm, value:0
	End Node:4(10,22):E
	Node:ListRules, value:1
		Node:OneRule, value:4
			End Node:6(43,44):<
			End Node:3(44,45):E
			Node:Stmt_RHS, value:5
				Node:Stmt_RHSTerm, value:9
					End Node:6(47,48):<
					Node:RHS_Stmt_Factor, value:15
						End Node:3(48,49):T
					Node:RestListRHSTerm_Stmt, value:11
						Node:RHS_Stmt_Factor, value:15
							End Node:3(50,52):E'
						Node:RestListRHSTerm_Stmt, value:12
					End Node:7(52,53):>
				Node:Stmt_RestListRHS, value:7
			End Node:7(53,54):>
		Node:RestListRules, value:2
			Node:OneRule, value:4
				End Node:6(75,76):<
				End Node:3(76,78):E'
				Node:Stmt_RHS, value:5
					Node:Stmt_RHSTerm, value:9
						End Node:6(79,80):<
						Node:RHS_Stmt_Factor, value:14
							End Node:2(80,81):+
						Node:RestListRHSTerm_Stmt, value:11
							Node:RHS_Stmt_Factor, value:15
								End Node:3(82,83):T
							Node:RestListRHSTerm_Stmt, value:11
								Node:RHS_Stmt_Factor, value:15
									End Node:3(84,86):E'
								Node:RestListRHSTerm_Stmt, value:12
						End Node:7(86,87):>
					Node:Stmt_RestListRHS, value:6
						Node:Stmt_RHSTerm, value:9
							End Node:6(88,89):<
							Node:RHS_Stmt_Factor, value:16
							Node:RestListRHSTerm_Stmt, value:12
							End Node:7(89,90):>
						Node:Stmt_RestListRHS, value:7
				End Node:7(90,91):>
			Node:RestListRules, value:2
				Node:OneRule, value:4
					End Node:6(93,94):<
					End Node:3(94,95):T
					Node:Stmt_RHS, value:5
						Node:Stmt_RHSTerm, value:9
							End Node:6(97,98):<
							Node:RHS_Stmt_Factor, value:15
								End Node:3(98,99):F
							Node:RestListRHSTerm_Stmt, value:11
								Node:RHS_Stmt_Factor, value:15
									End Node:3(100,102):T'
								Node:RestListRHSTerm_Stmt, value:12
							End Node:7(102,103):>
						Node:Stmt_RestListRHS, value:7
					End Node:7(103,104):>
				Node:RestListRules, value:2
					Node:OneRule, value:4
						End Node:6(105,106):<
						End Node:3(106,108):T'
						Node:Stmt_RHS, value:5
							Node:Stmt_RHSTerm, value:9
								End Node:6(109,110):<
								Node:RHS_Stmt_Factor, value:14
									End Node:2(110,111):*
								Node:RestListRHSTerm_Stmt, value:11
									Node:RHS_Stmt_Factor, value:15
										End Node:3(112,113):F
									Node:RestListRHSTerm_Stmt, value:11
										Node:RHS_Stmt_Factor, value:15
											End Node:3(114,116):T'
										Node:RestListRHSTerm_Stmt, value:12
								End Node:7(116,117):>
							Node:Stmt_RestListRHS, value:6
								Node:Stmt_RHSTerm, value:9
									End Node:6(139,140):<
									Node:RHS_Stmt_Factor, value:16
									Node:RestListRHSTerm_Stmt, value:12
									End Node:7(140,141):>
								Node:Stmt_RestListRHS, value:7
						End Node:7(141,142):>
					Node:RestListRules, value:2
						Node:OneRule, value:4
							End Node:6(143,144):<
							End Node:3(144,145):F
							Node:Stmt_RHS, value:5
								Node:Stmt_RHSTerm, value:9
									End Node:6(147,148):<
									Node:RHS_Stmt_Factor, value:14
										End Node:2(148,149):n
									Node:RestListRHSTerm_Stmt, value:12
									End Node:7(149,150):>
								Node:Stmt_RestListRHS, value:6
									Node:Stmt_RHSTerm, value:9
										End Node:6(151,152):<
										Node:RHS_Stmt_Factor, value:14
											End Node:2(152,153):(
										Node:RestListRHSTerm_Stmt, value:11
											Node:RHS_Stmt_Factor, value:15
												End Node:3(154,155):E
											Node:RestListRHSTerm_Stmt, value:10
												Node:RHS_Stmt_Factor, value:14
													End Node:2(156,157):)
												Node:RestListRHSTerm_Stmt, value:12
										End Node:7(157,158):>
									Node:Stmt_RestListRHS, value:7
							End Node:7(158,159):>
						Node:RestListRules, value:3

```

# Вывод
В данной лабораторной работе был реализован синтаксический анализатор на основе таблицы предсказвающего 
анализа.

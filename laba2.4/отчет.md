% Лабораторная работа № 2.4 «Множества FIRST для РБНФ»
% 25 апреля 2023 г.
% Головкин Дмитрий, ИУ9-62Б

# Цель работы
Целью данной работы является изучение алгоритма построения множеств FIRST для расширенной формы Бэкуса-Наура

# Индивидуальный вариант
' здесь очень интересный
' синтаксис для альтернатив

<E    <T {<
            <+>
            <->
          > T}>>
<T    <F {< 
            <*> 
            </>
          > F}>>
<F    <n>
      <- F>
      <( E )>>

# Реализация

## Неформальное описание синтаксиса входного языка
Синтаксис в неком смысле похож на лабораторную 2.3, но все же есть отличия. Отсутствует
 объявление аксимомы грамматики, считается что ей ялвяется первый нетерминал. Также присутсвуют
 комментарии и отдельного внимания заслуживают альтернативы. В данном случае они задаются через
 знак табуляции, но через него же задается и переход к другому правилу. Из-за этого возникли
 некоторые проблемы.

## Лексическая структура
list Rules
Rule = lhs \t rhs

## Грамматика языка
<Grammar> :: AXIOM <Rules>
<Rules> :: <Rule> <RestRules>
<RestRules> :: <Rule> <RestRules> | Epsilon
<Rule> :: <Lhs> <Rhs>
<Rhs> :: <RhsPart> <RhsRestParts>
<RhsPart> :: Term | NonTerm
<Lhs> :: NonTerm
<RhsRestParts> :: <RhsPart> <RhsRestParts> | Epsilon

## Программная реализация

```python
import re
from dataclasses import dataclass
from queue import Queue
from typing import List, Optional, Union
from collections import defaultdict


class Lex_Tags():
    Tag_ERROR = 1
    Tag_TINAL = 2
    Tag_NONTINAL = 3
    Tag_AXIOM = 4
    Tag_COMMENT = 5
    Tag_OPEN_GROUP = 6
    Tag_CLOSE_GROUP = 7
    Tag_START_Iter = 8
    Tag_End_Iter = 9
    Tag_CONCAT_OP = 10
    Tag_SEPARATOR_SYM = 11
    Tag_UNMATCHED = 12
    Tag_END = 13


class T:
    
    def __init__(self, tag = None, value = ""):
        self.tag = tag
        self.text = value

class CT(T):

    def __init__ (self, tag = None, value = "", index_start = 0, index_end = 0):
        self.tag = tag
        self.text = value
        self.index_start = index_start
        self.index_end = index_end

    def __repr__(self):
        return str(self.tag) + '(' + str(self.index_start) + ',' + str(self.index_end) + '):' + str(self.text)

class Lexer:

    def T_matcher(self, text_input):

        matched = re.match(r"'.*\n", text_input)
        if matched:
            return T(tag=Lex_Tags.Tag_COMMENT, value=matched.group())
        matched = re.match(r"<axiom <({})>>\n".format(r"[A-Z]'?"), text_input)
        if matched:
            return T(tag=Lex_Tags.Tag_AXIOM, value=matched.group())
        matched = re.match("<", text_input)
        if matched:
            return T(tag=Lex_Tags.Tag_OPEN_GROUP, value=matched.group())
        matched = re.match(">", text_input)
        if matched:
            return T(tag=Lex_Tags.Tag_CLOSE_GROUP, value=matched.group())
        matched = re.match(r"[a-z\(\)\+\*]",text_input)
        if matched:
            return T(tag=Lex_Tags.Tag_TINAL, value=matched.group())
        matched = re.match(r"[A-Z]'?", text_input)
        if matched:
            return T(tag=Lex_Tags.Tag_NONTINAL, value=matched.group())
        matched = re.match("{", text_input)
        if matched:
            return T(tag=Lex_Tags.Tag_START_Iter, value=matched.group())
        matched = re.match("}", text_input)
        if matched:
            return T(tag=Lex_Tags.Tag_End_Iter, value=matched.group())
        matched = re.match("\n", text_input)
        if matched:
            return T(tag=Lex_Tags.Tag_SEPARATOR_SYM, value=matched.group())
        matched = re.match("", text_input)
        if matched:
            return T(tag=Lex_Tags.Tag_CONCAT_OP, value="")

        return T(tag=Lex_Tags.Tag_UNMATCHED, value=text_input[0])

    def perform_Tize(self, text_input):
        new_Ts = Queue()
        idx = 0

        while idx < len(text_input):
            T = self.T_matcher(text_input[idx:])
            if T.tag == Lex_Tags.Tag_UNMATCHED and T.text.isspace() or T.tag == Lex_Tags.Tag_COMMENT:
                idx += len(T.text)
            else:
                if T.tag == Lex_Tags.Tag_AXIOM:
                    axiom_value = re.search(r"[A-Z]'?", text_input).group(0)
                    t = CT(Lex_Tags.Tag_AXIOM, axiom_value, idx, idx + len(T.text))
                    new_Ts.put(CT(Lex_Tags.Tag_AXIOM, axiom_value, idx, idx + len(T.text)))
                else:
                    new_Ts.put(CT(T.tag, T.text, idx, idx + len(T.text)))
                idx += len(T.text)

        new_Ts.put(CT(Lex_Tags.Tag_END, "", idx+1, idx+1))
        return new_Ts



class E:

    def __init__(self, value = ""):
        self.value = value
    
test_str = """<E    <T {<
            <+>
            <->
          > T}>>
<T    <F {< 
            <*> 
            </>
          > F}>>
<F    <n>
      <- F>
      <( E )>>"""

    
class GroupNode:
    def __init__(self, body = Optional[None]):
        self.body = body

class NonTinal:
    def __init__(self, str, id=0):
        self.value = str
        self.num_rule = id
        self.accessors = []

    def __hash__(self):
        return hash(self.value)

    def __str__(self):
        return self.value
    
    def print(self, indent):
        print(indent + str(self) + ":")
        for child in self.accessors:
            child.print(indent + "\t")

    def __eq__(self, other):
        return (isinstance(other, NonTinal) and
                self.value == other.value)

class IN:
    def __init__ (self, body = Optional[None]):
        self.body = body

class Tinal:
    def __init__(self, str):
        self.value = str

    def __hash__(self):
        return hash(self.value)

    def __str__(self):
        return self.value
    
    def print(self, indent):
        print(indent + str(self))

    def __eq__(self, other):
        return (isinstance(other, Tinal) and
                self.value == other.value)

    
class AN:
    def __init__(self, nodes):
        self.nodes = nodes


class R:
    def __init__(self, nodes):
        self.nodes = nodes


class Rule:
    def __init__(self, lhs, R):
        self.lhs = lhs
        self.R = R


class Parser:

    def parse(self, Ts):
        new_nonTinals = set()

        def programm():
            return R_list_R([one_rule()])

        def R_list_R(R):
            if Ts.queue[0].tag == Lex_Tags.Tag_SEPARATOR_SYM:
                Ts.get()
                R.append(one_rule())
                return R_list_R(R)
            return R

        def one_rule():
            assert Ts.get().tag == Lex_Tags.Tag_OPEN_GROUP
            lhs_R = Ts.get()
            assert lhs_R.tag == Lex_Tags.Tag_NONTINAL
            new_nonTinals.add(NonTinal(lhs_R.value))
            R_ = R_R()
            assert Ts.get().tag == Lex_Tags.Tag_CLOSE_GROUP
            return Rule(NonTinal(lhs_R.text), R_)

        def R_R():
            return R(R_list_R_R([R_list_T()]))

        def R_list_R_R(Ts):
            if Ts.queue[0].tag == Lex_Tags.Tag_SEPARATOR_SYM:
                Ts.get()
                Ts.append(R_list_T())
                return R_list_R_R(Ts)
            return Ts

        def R_list_T():
            return AN(R_list_R_T([R_factor_R()]))

        def R_list_R_T(factors):
            if Ts.queue[0].tag == Lex_Tags.Tag_CONCAT_OP:
                Ts.get()
                factors.append(R_factor_R())
                return R_list_R_T(factors)
            if (Ts.queue[0].tag == Lex_Tags.Tag_TINAL or 
               Ts.queue[0].tag == Lex_Tags.Tag_NONTINAL or
               Ts.queue[0].tag == Lex_Tags.Tag_OPEN_GROUP or
               Ts.queue[0].tag == Lex_Tags.Tag_START_Iter):
            
                factors.append(R_factor_R())
                return R_list_R_T(factors)
            return factors

        def R_factor_R():
            if Ts.queue[0].tag == Lex_Tags.Tag_CLOSE_GROUP:
                return E()
            T = Ts.get()
            
            if T.tag == Lex_Tags.Tag_SEPARATOR_SYM:
                T = Ts.get()
            if T.tag == Lex_Tags.Tag_TINAL:
                return Tinal(T.value)
            if T.tag == Lex_Tags.Tag_NONTINAL:
                nonTinals.add(NonTinal(T.value))
                return NonTinal(T.value)
            if T.tag == Lex_Tags.Tag_OPEN_GROUP:
                R_ = R_R()
                assert Ts.get().tag == Lex_Tags.Tag_CLOSE_GROUP
                return GroupNode(R_)
            if T.tag == Lex_Tags.ITERSTART:
                R_ = R_R()
                assert Ts.get().tag == Lex_Tags.Tag_End_Iter
                return IN(R_)

        R_ = R()
        return R_, nonTinals
    
lexer = Lexer()
Ts = lexer.perform_Tize(test_str)
R, nonTinals = Parser().parse(Ts)


def first(R, nonTinals):
    R_by_nts = defaultdict(list)

    for rule in R:
        R_by_nts[rule.lhs].append(rule)

    def first_rec(curr, first_set):
        if isinstance(curr, R):
            for node in curr.nodes:
                first_set |= first_rec(node, first_set)
            return first_set
        elif isinstance(curr, AN):
            return first_set | first_rec(curr.nodes[0], first_set)
        elif isinstance(curr, IN):
            return first_set | E() | first_rec(curr.value, first_set)
        elif isinstance(curr, GroupNode):
            return first_set | first_rec(curr.value, first_set)
        elif isinstance(curr, Tinal):
            first_set.add(curr)
            return first_set
        elif isinstance(curr, NonTinal):
            for rule in R_by_nts[curr]:
                first_set |= first_rec(rule.R, first_set)
            return first_set
        
    for nt in nonTinals:
        print(nt, "- FIRST:", first_rec(R_by_nts[nt][0].R, set()))
first(R, nonTinals)
```

Вывод на `stdout`

```
Nonterminal(symbol='F') - FIRST: {Terminal(symbol='-'), Terminal(symbol='n'), Terminal(symbol='(')}
Nonterminal(symbol='E') - FIRST: {Terminal(symbol='-'), Terminal(symbol='n'), Terminal(symbol='(')}
Nonterminal(symbol='T') - FIRST: {Terminal(symbol='-'), Terminal(symbol='n'), Terminal(symbol='(')}
```

# Вывод
В данной лабораторной работе был изучен алгоритм построения множеств First и Follow для грамматики, 
записанной в РБНФ.
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum, auto\n",
    "from queue import Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tag(Enum):\n",
    "    ERROR = auto()\n",
    "    TERM = auto()\n",
    "    NTERM = auto()\n",
    "    AXIOM = auto()\n",
    "    COMMENT = auto()\n",
    "    OPEN = auto()\n",
    "    CLOSE = auto()\n",
    "    ITERSTART = auto()\n",
    "    ITEREND = auto()\n",
    "    SEPARATOR = auto()\n",
    "    GROUPSTART = auto()\n",
    "    GROUPEND = auto()\n",
    "    ASSIGNMENT = auto()\n",
    "    ALT = auto()\n",
    "    CONCAT = auto()\n",
    "    UNMATCHED = auto()\n",
    "    END = auto()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Token:\n",
    "    \"\"\"Represents tokens for lexical analysis.\"\"\"\n",
    "\n",
    "    tag: Tag   \n",
    "    value: str = \"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CoordsToken(Token):\n",
    "    start_idx: int = 0\n",
    "    end_idx: int = 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.tag} ({self.start_idx}, {self.end_idx}): {self.value}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"open\": \"<\",\n",
    "    \"close\": \">\",\n",
    "    \"groupstart\": \"|\",\n",
    "    \"groupend\": \"|\",\n",
    "    \"iter_start\": \"{\",\n",
    "    \"iter_end\": \"}\",\n",
    "    \"concat\": \"\",\n",
    "    \"assignment\" : \"\\t\",\n",
    "    \"alt\": \"\\n\",\n",
    "    \"terminal\": r\"[a-z\\/\\-\\+\\*\\(\\)]\",\n",
    "    \"nonterminal\": r\"[A-Z]'?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lexer:\n",
    "    \"\"\"Performs lexical analysis of input data.\"\"\"\n",
    "\n",
    "    def __init__(self, config: dict):\n",
    "        self.config = config\n",
    "        self._re_mapping = {\n",
    "            r\"'.*\\n\": Tag.COMMENT,\n",
    "            r\"<axiom <({})>>\\n\".format(config[\"nonterminal\"]): Tag.AXIOM,\n",
    "            config[\"open\"]: Tag.OPEN,\n",
    "            config[\"close\"]: Tag.CLOSE,\n",
    "            config[\"iter_start\"]: Tag.ITERSTART,\n",
    "            config[\"iter_end\"]: Tag.ITEREND,\n",
    "            config[\"terminal\"]: Tag.TERM,\n",
    "            config[\"alt\"]: Tag.ALT,\n",
    "           # config[\"groupstart\"]: Tag.GROUPSTART,\n",
    "           # config[\"groupend\"]: Tag.GROUPEND,\n",
    "            config[\"assignment\"]: Tag.ASSIGNMENT,\n",
    "            #config[\"concat\"]: Tag.CONCAT,\n",
    "            config[\"nonterminal\"]: Tag.NTERM,   \n",
    "        }\n",
    "\n",
    "    def _match_token(self, input_str: str) -> Token:\n",
    "        for pattern, tag in self._re_mapping.items():\n",
    "            matched = re.match(pattern, input_str)\n",
    "            if matched:\n",
    "                return Token(tag=tag, value=matched.group())\n",
    "\n",
    "        return Token(tag=Tag.UNMATCHED, value=input_str[0])\n",
    "\n",
    "    def tokenize(self, input_str: str) -> Queue:\n",
    "        tokens = Queue()\n",
    "        idx = 0\n",
    "        open_tags = 0\n",
    "\n",
    "        while idx < len(input_str):\n",
    "            token = self._match_token(input_str[idx:])\n",
    "            if token.tag == Tag.UNMATCHED and token.value.isspace() or token.tag == Tag.COMMENT:\n",
    "                idx += len(token.value)\n",
    "            else:\n",
    "                if token.tag == Tag.AXIOM:\n",
    "                    axiom_value = re.search(self.config[\"nonterminal\"], token.value).group(0) # type: ignore\n",
    "                    tokens.put(CoordsToken(Tag.AXIOM, axiom_value, idx, idx + len(token.value)))\n",
    "                else:\n",
    "                    if token.tag == Tag.OPEN:\n",
    "                        open_tags += 1\n",
    "                    if token.tag == Tag.CLOSE:\n",
    "                        open_tags -= 1\n",
    "                    if token.tag == Tag.ALT and open_tags == 0:\n",
    "                        token.tag = Tag.SEPARATOR\n",
    "                    tokens.put(CoordsToken(token.tag, token.value, idx, idx + len(token.value)))\n",
    "                idx += len(token.value)\n",
    "\n",
    "        tokens.put(CoordsToken(Tag.END, \"\", idx+1, idx+1))\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexer = Lexer(config)\n",
    "\n",
    "test_str = \"\"\"<E\\t<T {<\n",
    "            <+>\n",
    "            <->\n",
    "          > T}>>\n",
    "<T\\t<F {< \n",
    "            <*> \n",
    "            </>\n",
    "          > F}>>\n",
    "<F\\t<n>\n",
    "      <- F>\n",
    "      <( E )>>\"\"\"\n",
    "\n",
    "tokens = lexer.tokenize(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while not (tokens.empty()):\n",
    "#     print(tokens.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Empty:\n",
    "    symbol: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Terminal:\n",
    "    symbol: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Nonterminal:\n",
    "    symbol: str\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, Nonterminal) and self.symbol == other.symbol\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.symbol)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GroupNode:\n",
    "    value: Optional[None]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OptionalNode:\n",
    "    value: Optional[None]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class IterNode:\n",
    "    value: Optional[None]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AltNode:\n",
    "    nodes: List[Union[Empty, Terminal, Nonterminal, GroupNode, OptionalNode, IterNode]]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RHS:\n",
    "    nodes: List[AltNode]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Rule:\n",
    "    lhs: Nonterminal\n",
    "    rhs: RHS\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CFGRule:\n",
    "    lhs: Nonterminal\n",
    "    rhs: List[Union[Empty, Terminal, Nonterminal]]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CFGrammar:\n",
    "    start: Nonterminal\n",
    "    rules: List[CFGRule]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    \"\"\"Performs syntax analysis of input data.\"\"\"\n",
    "\n",
    "    def parse(self, tokens: Queue):\n",
    "        nonterminals = set()\n",
    "\n",
    "        def rules():\n",
    "            return rest_rules([rule()])\n",
    "\n",
    "        def rest_rules(rules):\n",
    "            if tokens.queue[0].tag == Tag.SEPARATOR:\n",
    "                tokens.get()\n",
    "                rules.append(rule())\n",
    "                return rest_rules(rules)\n",
    "            return rules\n",
    "\n",
    "        def rule():\n",
    "            assert tokens.get().tag == Tag.OPEN\n",
    "            lhs = tokens.get()\n",
    "            #print(lhs)\n",
    "            assert lhs.tag == Tag.NTERM\n",
    "            nonterminals.add(Nonterminal(lhs.value))\n",
    "            #assert tokens.get().tag == Tag.CLOSE\n",
    "            assert tokens.get().tag == Tag.ASSIGNMENT\n",
    "            rhs_ = rhs()\n",
    "            return Rule(Nonterminal(lhs.value), rhs_)\n",
    "\n",
    "        def rhs():\n",
    "            return RHS(rest_rhs([rhs_term()]))\n",
    "\n",
    "        def rest_rhs(terms):\n",
    "            if tokens.queue[0].tag == Tag.ALT:\n",
    "                tokens.get()\n",
    "                terms.append(rhs_term())\n",
    "                return rest_rhs(terms)\n",
    "            return terms\n",
    "\n",
    "        def rhs_term():\n",
    "            return AltNode(rest_rhs_term([rhs_factor()]))\n",
    "\n",
    "        def rest_rhs_term(factors):\n",
    "            if tokens.queue[0].tag == Tag.CONCAT:\n",
    "                tokens.get()\n",
    "                factors.append(rhs_factor())\n",
    "                return rest_rhs_term(factors)\n",
    "            if tokens.queue[0].tag in [\n",
    "                #Tag.EPS,\n",
    "                Tag.TERM,\n",
    "                Tag.OPEN,\n",
    "                Tag.GROUPSTART,\n",
    "                Tag.ITERSTART,\n",
    "            ]:\n",
    "                factors.append(rhs_factor())\n",
    "                return rest_rhs_term(factors)\n",
    "            return factors\n",
    "\n",
    "        def rhs_factor():\n",
    "            token = tokens.get()\n",
    "            # if token.tag == Tag.EPS:\n",
    "            #     return Empty(token.value)\n",
    "            if token.tag == Tag.TERM:\n",
    "                return Terminal(token.value)\n",
    "            # if token.tag == Tag.OPEN:\n",
    "            #     token = tokens.get()\n",
    "            #     print(token.tag, '11')\n",
    "            #     assert token.tag == Tag.NTERM\n",
    "            #    # print(tokens.get().tag, '22')\n",
    "            #     token = tokens.get()\n",
    "            #    # assert tokens.tag == Tag.END\n",
    "            #     nonterminals.add(Nonterminal(token.value))\n",
    "            #     return Nonterminal(token.value)\n",
    "            if token.tag == Tag.ALT:\n",
    "                rhs_ = rhs()\n",
    "                assert tokens.get().tag == Tag.GROUPEND\n",
    "                return GroupNode(rhs_)\n",
    "            if token.tag == Tag.ITERSTART:\n",
    "                rhs_ = rhs()\n",
    "                assert tokens.get().tag == Tag.ITEREND\n",
    "                return IterNode(rhs_)\n",
    "            raise Exception(f\"Wrong Factor <{token.tag}> with value <{token.value}>\")\n",
    "\n",
    "        rules_ = rules()\n",
    "        return rules_, nonterminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Wrong Factor <Tag.OPEN> with value <<>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-312-8442d09dfc3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnont\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-311-ecbbcf6948b2>\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Wrong Factor <{token.tag}> with value <{token.value}>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mrules_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrules_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonterminals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-311-ecbbcf6948b2>\u001b[0m in \u001b[0;36mrules\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mrest_rules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrest_rules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-311-ecbbcf6948b2>\u001b[0m in \u001b[0;36mrule\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m#assert tokens.get().tag == Tag.CLOSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mASSIGNMENT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mrhs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mRule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNonterminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-311-ecbbcf6948b2>\u001b[0m in \u001b[0;36mrhs\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mRHS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrest_rhs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrhs_term\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrest_rhs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-311-ecbbcf6948b2>\u001b[0m in \u001b[0;36mrhs_term\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrhs_term\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mAltNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrest_rhs_term\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrhs_factor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrest_rhs_term\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-311-ecbbcf6948b2>\u001b[0m in \u001b[0;36mrhs_factor\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITEREND\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mIterNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Wrong Factor <{token.tag}> with value <{token.value}>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mrules_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Wrong Factor <Tag.OPEN> with value <<>"
     ]
    }
   ],
   "source": [
    "rules, nont = Parser().parse(tokens=tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule(lhs=Nonterminal(symbol='E'), rhs=RHS(nodes=[AltNode(nodes=[Nonterminal(symbol='{')])]))\n"
     ]
    }
   ],
   "source": [
    "for rule in rules:\n",
    "    print(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
